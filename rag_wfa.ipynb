{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9bd467e-d3f3-4b39-9230-59453079897b",
   "metadata": {},
   "source": [
    "##  RAG on WFA Specifications using OpenAI Embeddings and Chroma Vector Store "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "This project uses RAG (Retrieval Augmented Generation) to build a question/answering assistant. I have used WFA specifications as base for these documents, but this can be expanded to any other set of documents.\n",
    "\n",
    "This project demonstrates how to build a conversational AI system using OpenAI's language model with memory and a vector-based retrieval mechanism for Retrieval-Augmented Generation (RAG). It enables context-aware, dynamic conversations with enhanced retrieval capabilities for domain-specific knowledge.\n",
    "\n",
    "-----------\n",
    "\n",
    "Features -\n",
    "\n",
    "Conversational Context Memory: Tracks the history of exchanges for continuity in multi-turn conversations.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG): Integrates a vector database for efficient retrieval of relevant documents or context.\n",
    "\n",
    "OpenAI GPT Integration: Leverages OpenAI's language models (e.g., GPT-3.5 or GPT-4) for generating high-quality responses.\n",
    "\n",
    "-----------\n",
    "\n",
    "Example -\n",
    "\n",
    "Input Prompt: what is GAS initial response frame\n",
    "\n",
    "Output: The GAS Initial Response Frame is a type of action frame used in the Generic Advertisement Service (GAS) as defined in the Wi-Fi Direct® Specification. It is part of the communication process between devices, particularly in peer-to-peer (P2P) networking scenarios.\n",
    "\n",
    "The frame includes several fields such as the Category field (which indicates it is a Public Action Frame), Action field (set to a specific value for a GAS Initial Response), Dialog Token (copied from the corresponding GAS Initial Request), and various other fields that provide details about the response, such as status codes, query response length, and any advertisement protocol information.\n",
    "\n",
    "In essence, the GAS Initial Response Frame is sent by a device in response to a GAS Initial Request from another device, facilitating the exchange of information related to services and capabilities available within the network.\n",
    "\n",
    "-----------\n",
    "\n",
    "This was better response than ChatGPT. When ChatGPT was asked \"what is GAS initial response frame\". It gave an irrelevant response:\n",
    " \n",
    "The GAS Initial Response Frame typically refers to the structured approach used in emergency response or incident management systems to establish an organized and effective initial reaction to emergencies or critical incidents. GAS might stand for a specific organization or methodology depending on the context (e.g., Gasoline Emergency Response or a General Assessment Strategy), but without additional context, I’ll outline a general framework often applied in emergency management scenarios:\n",
    "\n",
    "https://chatgpt.com/share/679543e5-9464-8000-975c-2142132bf737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a236757-b204-4249-b0da-ed873e68ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "#openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de967999-23c0-4385-9e1c-98db9266432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = PyPDFLoader(\"WFA Specifications\\Miracast_Specification_v2.3.pdf\")\n",
    "#pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b490be7-d85f-4e91-9493-a1eb3805de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd57ded-af45-4d62-99ab-d334e61a9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob.glob(\"WFA Specifications/*.pdf\")\n",
    "documents = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    doc_type = os.path.basename(pdf_file)\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pdf_docs = loader.load()\n",
    "    for doc in pdf_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba672f2-8374-4044-ae58-e7a2f6f3443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253dc59-34ff-41f2-a709-c6c891806786",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2551e-1165-4aec-9b30-97147bc0f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start embedding\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "if os.path.exists(\"wfa_database\"):\n",
    "    Chroma(persist_directory=\"wfa_database\", embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"wfa_database\")\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1472410-c091-4081-9657-cda689f7d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vectorstore with 1985 documents\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "if os.path.exists(\"wfa_database\"):\n",
    "    vectorstore = Chroma(persist_directory=\"wfa_database\", embedding_function=embeddings)\n",
    "    print(f\"Loaded existing vectorstore with {vectorstore._collection.count()} documents\")\n",
    "else:\n",
    "    vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"wfa_database\")\n",
    "    print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311bd20d-9244-4574-b24e-1017aa52a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1,985 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee56f1d2-71e4-409f-b820-e07c780048f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=1, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec2366cc-6888-4e5a-a1f1-8a5e026549ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not specify or categorize different types of WFD sessions. Therefore, I don't know the answer to your question.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the types of WFD sessions\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16c6dd96-c046-464e-80c0-0be10a4c9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48873d11-2fbd-4329-af27-46c781788561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8db1a88-6afb-4cde-9e0e-ef2d58e1c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\", title=\"WFA Assistant\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a38d4-4824-4366-a549-d75aacd2d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9a434-b1d3-4bac-93e3-48b410ae11f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
